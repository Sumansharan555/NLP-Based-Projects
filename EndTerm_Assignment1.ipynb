{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXP_wFddx0Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 1: Using a small text dataset (e.g., tweets, reviews, or articles), build a corpus and apply at least three preprocessing techniques‚Äîsuch as tokenization, stopword removal, and stemming/lemmatization. Present the processed results with explanations."
      ],
      "metadata": {
        "id": "0y7XSkZQx01w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sfMVUzBykcL"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"I love this movie! It's fantastic üòç\",\n",
        "    \"This film was terrible... I hated it!\",\n",
        "    \"Amazing storyline and great acting.\",\n",
        "    \"Worst movie ever. Waste of time.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Use PyPDF2 to extract text from page 2 of Business_Proposal.pdf¬∂\n",
        "import PyPDF2\n",
        "\n",
        "with open(\"Business_Proposal.pdf\", \"rb\") as pdf_file:\n",
        "    reader = PyPDF2.PdfReader(pdf_file)\n",
        "    page_two_text = reader.pages[1].extract_text()\n",
        "\n",
        "print(\"\\nExtracted text from Business_Proposal.pdf (Page 2):\")\n",
        "print(page_two_text)"
      ],
      "metadata": {
        "id": "qg_Y6TXiBlcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "HcAoXR4xzOYY",
        "outputId": "cfc0f7de-31bf-44b5-c02d-028e0f2590c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\n\\n# Tokenization\\ntokens = [word_tokenize(text.lower()) for text in texts]\\n\\n# Stopword Removal\\nstop_words = set(stopwords.words(\\'english\\'))\\nfiltered_tokens = [[w for w in t if w.isalpha() and w not in stop_words] for t in tokens]\\n\\n# Stemming\\nstemmer = PorterStemmer()\\nstemmed = [[stemmer.stem(w) for w in t] for t in filtered_tokens]\\n\\n# Lemmatization\\nlemmatizer = WordNetLemmatizer()\\nlemmatized = [[lemmatizer.lemmatize(w) for w in t] for t in filtered_tokens]\\n\\nprint(\"Original:\", texts[0])\\nprint(\"Tokens:\", tokens[0])\\nprint(\"Filtered:\", filtered_tokens[0])\\nprint(\"Stemmed:\", stemmed[0])\\nprint(\"Lemmatized:\", lemmatized[0])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_hfrbXAzfEj",
        "outputId": "013e5df7-e013-4c67-ff3a-7627a6d7d521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenization\n",
        "tokens = [word_tokenize(text.lower()) for text in texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTXKxP3fziU8",
        "outputId": "37051b61-679a-4b8b-8fe8-98b1d2d3a009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [[w for w in t if w.isalpha() and w not in stop_words] for t in tokens]\n"
      ],
      "metadata": {
        "id": "B4a_FUwEzn_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [[stemmer.stem(w) for w in t] for t in filtered_tokens]\n"
      ],
      "metadata": {
        "id": "OsQKBLGHz_Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [[lemmatizer.lemmatize(w) for w in t] for t in filtered_tokens]\n"
      ],
      "metadata": {
        "id": "JBpzCUlz0C7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original:\", texts[0])\n",
        "print(\"Tokens:\", tokens[0])\n",
        "print(\"Filtered:\", filtered_tokens[0])\n",
        "print(\"Stemmed:\", stemmed[0])\n",
        "print(\"Lemmatized:\", lemmatized[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60P9djSt0G8C",
        "outputId": "3ef4586b-2bb5-419a-ab4a-b003cfbd24ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I love this movie! It's fantastic üòç\n",
            "Tokens: ['i', 'love', 'this', 'movie', '!', 'it', \"'s\", 'fantastic', 'üòç']\n",
            "Filtered: ['love', 'movie', 'fantastic']\n",
            "Stemmed: ['love', 'movi', 'fantast']\n",
            "Lemmatized: ['love', 'movie', 'fantastic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozaXW-ad0K6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}