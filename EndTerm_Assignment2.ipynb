{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKsq6akJ0pPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 2: Implement preprocessing methods such as tokenization, stemming, lemmatization, and stopword removal to clean textual data. Compare different text representation models like Bag-of-Words, TF-IDF, and word embeddings (Word2Vec, GloVe)."
      ],
      "metadata": {
        "id": "O4MJadzQ00dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is powerful\",\n",
        "    \"I love AI\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "l4g_hnpw01Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow = CountVectorizer()\n",
        "bow_features = bow.fit_transform(documents)\n",
        "\n",
        "print(\"BoW Features:\\n\", bow_features.toarray())\n",
        "print(\"Vocabulary:\", bow.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s6LyesU1D0x",
        "outputId": "863ff6d6-efdf-4cb1-cf20-f8bbb62897f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Features:\n",
            " [[0 0 1 1 1 0]\n",
            " [0 1 1 0 1 1]\n",
            " [1 0 0 1 0 0]]\n",
            "Vocabulary: ['ai' 'is' 'learning' 'love' 'machine' 'powerful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_features = tfidf.fit_transform(documents)\n",
        "\n",
        "print(\"TF-IDF Features:\\n\", tfidf_features.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhZwVWhM1JQq",
        "outputId": "d425fa41-c161-4364-a8ee-faea6c0139ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Features:\n",
            " [[0.         0.         0.57735027 0.57735027 0.57735027 0.        ]\n",
            " [0.         0.5628291  0.42804604 0.         0.42804604 0.5628291 ]\n",
            " [0.79596054 0.         0.         0.60534851 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "tokenized_texts = [t.split() for t in documents]\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=3, min_count=1, workers=2)\n",
        "print(\"Word2Vec vector for 'machine':\", w2v_model.wv['machine'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4TlLiGB1UC3",
        "outputId": "6f2807e4-63ac-4f71-910b-0a73926da7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec vector for 'machine': [ 0.00855287  0.00015212 -0.01916856 -0.01933109 -0.01229639 -0.00025714\n",
            "  0.00399483  0.01886394  0.0111687  -0.00858139  0.00055663  0.00992872\n",
            "  0.01539662 -0.00228845  0.00864684 -0.01162876 -0.00160838  0.0162001\n",
            " -0.00472013 -0.01932691  0.01155852 -0.00785964 -0.00244575  0.01996103\n",
            " -0.0045127  -0.00951413 -0.01065877  0.01396178 -0.01141774  0.00422733\n",
            " -0.01051132  0.01224143  0.00871461  0.00521271 -0.00298217 -0.00549213\n",
            "  0.01798587  0.01043155 -0.00432504 -0.01894062 -0.0148521  -0.00212748\n",
            " -0.00158989 -0.00512582  0.01936544 -0.00091704  0.01174752 -0.01489517\n",
            " -0.00501215 -0.01109973]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pH0camfF1oUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}